#!/usr/bin/python

from lex import Lexer
import sys
from parser import Parser


class Glex:
    def __init__(self, t):
        self.t=t
        self.i=0
        self.line_index=[]
        x=0
        while True:
            x=t.find("\n",x+1)
            if x == -1: break
            print x
            self.line_index.append(x)
            
        
        self.kws=[':','(',')','+','|','*','?',':=','[]','+=',';','.', 'class', 'return','construct']
        self.kws.sort(key=lambda x: -len(x))
        print self.kws
        
    def nextToken(self):
        for x in self.kws:
            self.t[i:].startswith(x)
            

    def nextRegexp(self):
        pass




target=sys.argv[1]

lexer = Lexer()
parser = Parser()
#Parse each line in the gramme spec
print sys.argv[2]
# for line in open(sys.argv[2], "r"):
#     #Remove comments
#     i=0
#     while i < len(line):
#         if line[i] == '\\':
#             i += 2
#         elif line[i] == '#':
#             break
#         else:
#             i += 1
#     line = line[:i].strip()
#     if not line: continue
#     name, d = map(str.strip, line.split(":=",1))
#     if name[0].isupper():
#         #Add a lexer rule
#         lexer.addClass(name, d)
#     else:
#         #Lex the gramma rule
#         toks = parser.lex(d)
        
#         #Replace all keywords with token identifiers
#         for i in range(len(toks)):
#             if toks[i][0] != parser.KEYWORD: continue
#             toks[i] = (parser.TLIT, lexer.addKeyword(toks[i][1]))
        
#         parser.addRule(toks)

l=Glex(open(sys.argv[2], "r").read())
s = ["#Parser lexer auto generated by mycc, DO NOT EDIT",""]
s += lexer.generate(target)
s += parser.generate(target)
s += [""]
open(sys.argv[3], "w").write("\n".join(s))
